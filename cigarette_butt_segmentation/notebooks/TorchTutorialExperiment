{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"TorchTutorialExperiment","provenance":[{"file_id":"1U6sVtVVeq5Pd7eCq1YKZnUk_VH00CgB7","timestamp":1601675524738}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"12eTZAm86zk9","executionInfo":{"status":"ok","timestamp":1601742812300,"user_tz":-420,"elapsed":1944,"user":{"displayName":"MikFolding","photoUrl":"","userId":"18160343468927980342"}},"outputId":"e986a6c9-b5cf-40b3-994b-367a86adadeb","colab":{"base_uri":"https://localhost:8080/","height":428}},"source":["!nvidia-smi\n","\n","import os\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive')\n","\n","work_root = \"/content/gdrive/My Drive/cigarette_butt_segmentation\"\n","%cd \"$work_root\"\n","data_root = '/content/data'\n","if not os.path.exists(data_root):\n","    !unzip data/cig_butts.zip -d $data_root\n","    !mv $data_root/cig_butts/* $data_root\n","    !rm -r $data_root/cig_butts \n","out_dir = work_root + '/results'\n","weights_file_name_templ = out_dir + '/CigHeadWeights_Epoch%d.h5'\n","\n","if 0:       # Warning: one-time operation, then the sources are supposed to be edited\n","    %cd /content\n","    !git clone https://github.com/pytorch/vision.git\n","    %cd vision\n","    !git checkout v0.3.0\n","\n","    target_dir = work_root + \"/lib/detection\"\n","    %mkdir \"$target_dir\"\n","    # # !cp references/detection/utils.py ../\n","    # # !cp references/detection/transforms.py ../\n","    # !cp references/detection/coco_eval.py \"$work_root/lib/torchvision/\"\n","    # !cp references/detection/engine.py \"$work_root/lib/torchvision/\"\n","    # # !cp references/detection/coco_utils.py ../\n","    !cp references/detection/*.py \"$target_dir/\"\n","\n","    %cd $work_root"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Sat Oct  3 16:33:36 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   43C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/My Drive/cigarette_butt_segmentation\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bUnpMSimwPW6","executionInfo":{"status":"ok","timestamp":1601742813703,"user_tz":-420,"elapsed":3262,"user":{"displayName":"MikFolding","photoUrl":"","userId":"18160343468927980342"}}},"source":["import cv2\n","import json\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","from PIL import ImageFile\n","from glob import glob\n","\n","from lib import *\n","from lib.net import *\n","\n","# from lib.torchvision.transforms1 import get_transform\n","# from lib.torchvision import train\n","# from lib.torchvision import engine\n","# from lib.torchvision.utils import *\n","\n","# %ls -l lib/detection/\n","import lib.detection.transforms\n","from lib.detection.transforms import get_transform\n","from lib.detection import train\n","from lib.detection import engine\n","from lib.detection.utils import *\n","\n","%matplotlib inline\n","\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"KA27AA9IU9u0","executionInfo":{"status":"ok","timestamp":1601742813709,"user_tz":-420,"elapsed":3229,"user":{"displayName":"MikFolding","photoUrl":"","userId":"18160343468927980342"}}},"source":["# import torchvision.models.segmentation as segmentation\n","\n","# # model = segmentation.deeplabv3_resnet50(True, True, 2)\n","#     # Fails because it is pretrained on 21 classes\n","\n","# import torchvision\n","# from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n"," \n","# # load a model pre-trained on COCO\n","# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","\n","# # replace the classifier with a new one, that has\n","# # num_classes which is user-defined\n","# num_classes = 2  # 1 class (person) + background\n","# # get number of input features for the classifier\n","# in_features = model.roi_heads.box_predictor.cls_score.in_features\n","# # replace the pre-trained head with a new one\n","# model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"fyqUrAwiY2Md","executionInfo":{"status":"ok","timestamp":1601742813715,"user_tz":-420,"elapsed":3206,"user":{"displayName":"MikFolding","photoUrl":"","userId":"18160343468927980342"}},"outputId":"97add639-7e42-4584-ee1f-86e28fcb92ea","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["dataset = CigDataset(data_root + '/train', get_transform(train=True))\n","dataset_test = CigDataset(data_root + '/val', get_transform(train=False))\n","\n","# split the dataset in train and test set\n","torch.manual_seed(1)\n","# indices = torch.randperm(len(dataset)).tolist()\n","# dataset = torch.utils.data.Subset(dataset, indices[:-50])\n","# dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n","\n","# define training and validation data loaders\n","data_loader = torch.utils.data.DataLoader(\n","    dataset, batch_size=12, shuffle=True, num_workers=4,\n","    collate_fn=collate_fn)\n","\n","data_loader_test = torch.utils.data.DataLoader(\n","    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n","    collate_fn=collate_fn)\n","\n","# dataset[0]"],"execution_count":4,"outputs":[{"output_type":"stream","text":["2000 images\n","200 images\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oL1OEv2layHM","executionInfo":{"status":"ok","timestamp":1601742816988,"user_tz":-420,"elapsed":6451,"user":{"displayName":"MikFolding","photoUrl":"","userId":"18160343468927980342"}}},"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","num_classes = 2\n","model = get_instance_segmentation_model(num_classes, True)\n","model.to(device)\n","# criterion = train.get_criterion()\n","epochNum = 0\n","\n","# construct an optimizer\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.005,\n","                            momentum=0.9, weight_decay=0.0005)\n","\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n","                                               step_size=10,\n","                                               gamma=0.1)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"h3NIzEjXbZE0","outputId":"82c7e0be-6520-4b3b-fba8-5aba31903cc5","colab":{"base_uri":"https://localhost:8080/","height":268}},"source":["num_epochs = 100\n","\n","for epoch in range(num_epochs):\n","    train.train_one_epoch(model, optimizer, data_loader, device, epochNum, print_freq=10)\n","    # train.train_one_epoch(model, train.criterion, optimizer, data_loader, lr_scheduler, device,\n","    #                       epochNum, print_freq=10)\n","    # lr_scheduler.step()\n","    train.evaluate(model, data_loader_test, device=device)\n","    epochNum += 1\n","    if epochNum % 5 == 0:\n","        save_model_state(model.roi_heads.mask_predictor, weights_file_name_templ % epochNum)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3000: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n","/usr/local/lib/python3.6/dist-packages/torchvision/ops/boxes.py:101: UserWarning: This overload of nonzero is deprecated:\n","\tnonzero()\n","Consider using one of the following signatures instead:\n","\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n","  keep = keep.nonzero().squeeze(1)\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: [0]  [  0/167]  eta: 0:10:44  lr: 0.000035  loss: 2.8281 (2.8281)  loss_classifier: 0.3897 (0.3897)  loss_box_reg: 0.1265 (0.1265)  loss_mask: 2.2947 (2.2947)  loss_objectness: 0.0157 (0.0157)  loss_rpn_box_reg: 0.0015 (0.0015)  time: 3.8571  data: 1.8838  max mem: 9268\n","Epoch: [0]  [ 10/167]  eta: 0:04:07  lr: 0.000336  loss: 1.5595 (1.6780)  loss_classifier: 0.3472 (0.3075)  loss_box_reg: 0.1085 (0.1095)  loss_mask: 1.1175 (1.2399)  loss_objectness: 0.0192 (0.0198)  loss_rpn_box_reg: 0.0015 (0.0014)  time: 1.5787  data: 0.1973  max mem: 9542\n","Epoch: [0]  [ 20/167]  eta: 0:03:37  lr: 0.000637  loss: 0.6118 (1.1183)  loss_classifier: 0.1271 (0.2147)  loss_box_reg: 0.1232 (0.1218)  loss_mask: 0.3355 (0.7610)  loss_objectness: 0.0177 (0.0193)  loss_rpn_box_reg: 0.0015 (0.0015)  time: 1.3613  data: 0.0341  max mem: 9542\n","Epoch: [0]  [ 30/167]  eta: 0:03:18  lr: 0.000938  loss: 0.4271 (0.8840)  loss_classifier: 0.0998 (0.1723)  loss_box_reg: 0.1363 (0.1285)  loss_mask: 0.1713 (0.5660)  loss_objectness: 0.0082 (0.0157)  loss_rpn_box_reg: 0.0014 (0.0015)  time: 1.3820  data: 0.0416  max mem: 9542\n","Epoch: [0]  [ 40/167]  eta: 0:03:02  lr: 0.001239  loss: 0.3697 (0.7518)  loss_classifier: 0.0613 (0.1445)  loss_box_reg: 0.1486 (0.1349)  loss_mask: 0.1314 (0.4582)  loss_objectness: 0.0040 (0.0128)  loss_rpn_box_reg: 0.0013 (0.0014)  time: 1.3896  data: 0.0415  max mem: 9542\n","Epoch: [0]  [ 50/167]  eta: 0:02:46  lr: 0.001540  loss: 0.3194 (0.6650)  loss_classifier: 0.0538 (0.1254)  loss_box_reg: 0.1491 (0.1362)  loss_mask: 0.1181 (0.3912)  loss_objectness: 0.0028 (0.0108)  loss_rpn_box_reg: 0.0009 (0.0014)  time: 1.3838  data: 0.0385  max mem: 9542\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ISlyQ6xigvdu"},"source":["# save_model_state(model.roi_heads.mask_predictor, weights_file_name_templ % epochNum)\n","# save_model_state(model, (weights_file_name_templ % epochNum) + '_full')   # \n","# load_model_state(model.roi_heads.mask_predictor, weights_file_name_templ % 10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wsVNiWKDhx20"},"source":["img_idxs = [0, 1, 2]\n","rowCount = len(img_idxs)\n","colCount = 5\n","plt.figure(figsize=(12, 8))\n","model.eval()\n","with torch.no_grad():\n","    for img_idx2, img_idx in enumerate(img_idxs):\n","        img, target = dataset[img_idx]\n","        images = images_to_device(img, device)\n","        targets = targets_to_device(target, device)\n","        pred = model(images)\n","        masks = pred[0]['masks'].cpu()\n","        print('masks:', masks.shape)\n","        mask_sum = np.zeros(masks.shape[2:])\n","        for mask_ind in range(min(colCount, masks.shape[0])):\n","            # mask = masks[mask_ind, 0].mul((mask_ind + 1) * 10).byte().numpy()\n","            mask = masks[mask_ind, 0].numpy()\n","            where = np.where(mask > 0)\n","            print(mask.max(), where[0][0], where[1][0], len(where[0]))\n","            mask_sum[where] = mask_sum[where] * 0.7 + 0.3\n","            # mask_sum += mask * 0.3\n","            ax = plt.subplot(rowCount, colCount, img_idx2 * colCount + mask_ind + 1)\n","        # ax = plt.subplot(1, colCount, img_idx2 + 1)\n","            im = ax.imshow(mask) # [100:200, 300:400])\n","        # plt.colorbar(im, ax=ax)\n","        # ax.imshow(np.transpose(img, (1, 2, 0)))\n","\n","        # one_img_dataset = torch.utils.data.Subset(dataset, [img_idx])\n","        # one_img_data_loader = torch.utils.data.DataLoader(\n","        #         one_img_dataset, batch_size=1, shuffle=False, num_workers=1,\n","        #         collate_fn=collate_fn)\n","        # train.evaluate(model, one_img_data_loader, device=device)\n","# plt.show()\n","# pred\n","# Image.fromarray(pred[0]['masks'][0, 0].mul(255).byte().cpu().numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rytHcSeMwPXi"},"source":["# Данные, метрики и доступные функции"]},{"cell_type":"markdown","metadata":{"id":"uCeB_dCswPXl"},"source":["Посмотрим на данные:"]},{"cell_type":"code","metadata":{"id":"ZEvZJdDVwPXo"},"source":["def show_images(data_path, img_ids):\n","    images = os.listdir(f\"{data_path}/images\")\n","    annotations = json.load(open(f\"{data_path}/coco_annotations.json\", \"r\"))\n","    for img_id in img_ids:\n","        img = None\n","        for ext in ['png', 'jpg', 'jpeg', 'gif']:\n","            img_path = f\"{data_path}/images/{img_id:08}.{ext}\"\n","            if os.path.exists(img_path):\n","                img = np.array(Image.open(img_path))\n","        mask = utils.get_mask(img_id, annotations)  # [130:170, 270:350]\n","        show.show_img_with_mask(img, mask)\n","    return mask\n","    \n","mask = show_images(f\"{data_root}/val\", [3, 5])      # A couple of strange val. images"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2J4hcG0nwPX_"},"source":["Пример подсчета метрики:"]},{"cell_type":"code","metadata":{"id":"PVz6cAKRwPYB"},"source":["random_mask = np.random.randint(low=0, high=2, size=mask.shape)\n","get_dice(mask, random_mask), np.sum((mask > 0).astype(int)) / mask.size"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FjLGOU4HwPYP"},"source":["// Можно для последовательности масок вычислить среднее значение метрики\n"]},{"cell_type":"markdown","metadata":{"id":"QhAIh3ciwPYm"},"source":["Пример использования функций `encode_rle` и `decode_rle`:\n","1. Функция `encode_rle` используется для кодирования маски в строку для последующей записи в файл;\n","2. Функция `decode_rle` используется для восстановления маски по закодированной строке."]},{"cell_type":"code","metadata":{"id":"tY_q2gCQwPYo"},"source":["rle_mask = encode_rle(mask)\n","rle_mask"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"we_ei9xEwPZO"},"source":["# Результаты"]},{"cell_type":"markdown","metadata":{"id":"cDy809DHwPZR"},"source":["Пример файла для изображений из `data/valid`:  \n","_Каждую предсказанную маску для изображения из `valid` необходимо закодировать и записать в показанный ниже файл, который служит примером, именно в таком виде нужно будет представить результат Вашего лучшего решения на данных из `valid`._"]},{"cell_type":"code","metadata":{"id":"UaxlHGfmwPZU"},"source":["pred = pd.read_csv(\"data/pred_val_template.csv\")\n","pred.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G3q7A3j_wPZn"},"source":["Для данных из `test` требуется создать html страницу + картинки для нее.  \n","Это можно сделать с помощью функции `get_html`, как показано ниже."]},{"cell_type":"code","metadata":{"id":"hB8M3u3DwPZp"},"source":["val_annotations = json.load(open(f\"{data_root}/val/coco_annotations.json\", \"r\"))\n","paths_to_imgs = sorted(glob(f\"{data_root}/val/images/*\"))\n","paths_to_imgs = paths_to_imgs[:10]\n","img_ids = [int(path.split(\"/\")[-1].split(\".\")[0]) for path in paths_to_imgs]\n","masks = [get_mask(img_id, val_annotations) for img_id in sorted(img_ids)]\n","\n","path_to_save = \"results/example\"\n","# generate_images_for_html(paths_to_imgs, masks, path_to_save=path_to_save)     \n","# generate_html(path_to_save, 10)\n","\n","masks[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y3c3Js9KwPZ0"},"source":["В папке `results` создался файл `example.html` и папка `examples` с используемыми картинками."]},{"cell_type":"code","metadata":{"id":"UKIJQNZjwPZ2"},"source":[""],"execution_count":null,"outputs":[]}]}